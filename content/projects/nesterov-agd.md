---
title: "Nesterov's Accelerated Gradient Descent Method"
summary: "A tutorial paper on Nesterov's AGD method"
tags: ["machine learning", "numerical optimisation"]
date: "2024-05-06"
readTime: true
---

Nesterov's Accelerated Gradient Descent (AGD) method is a variant of gradient descent that incorporates acceleration- and momentum-based concepts to overcome the limitations found in its classical counterpart. In this tutorial paper, we will provide an intution introduction to the classical GD and momentum methods, see how they are incorporated into AGD, and compare how they perform against AGD on some toy problems. This is a tutorial paper that I wrote in [STAT3007](https://programs-courses.uq.edu.au/course.html?course_code=stat3007) in 2024 Semester 1.

---

* <a href="/projects/nesterov-agd-tute-paper/tutorial.pdf" target="_blank">Tutorial paper</a>
