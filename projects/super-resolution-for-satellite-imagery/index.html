<!doctype html><html lang=en-au dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://limaoc.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://limaoc.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://limaoc.dev/favicon-32x32.png><link rel=icon type=image/png sizes=192x192 href=https://limaoc.dev/android-chrome-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://limaoc.dev/apple-touch-icon.png><meta name=description content><title>Super Resolution for Satellite Imagery | Limao Chang</title><link rel=canonical href=https://limaoc.dev/projects/super-resolution-for-satellite-imagery/><meta property="og:url" content="https://limaoc.dev/projects/super-resolution-for-satellite-imagery/"><meta property="og:site_name" content="Limao Chang"><meta property="og:title" content="Super Resolution for Satellite Imagery"><meta property="og:description" content="Enhancing satellite imagery resolution with state-of-the-art machine learning vision models"><meta property="og:locale" content="en_au"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-06-02T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-02T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><link rel=stylesheet href=/assets/combined.min.92c3bf7119b98cfdc79e93f36a451eb901d8bbbfed7d75814e6436cf6c9085dc.css media=all></head><body class=auto><div class=content><header><div class=header><h1 class=header-title><a href=https://limaoc.dev/>Limao Chang</a></h1><div class=header-menu><p class=small><a href=/>/home</a></p><p class=small><a href=/projects>/projects</a></p><p class=small><a href=/resume>/resume</a></p><p class=small><a href=/tags>/tags</a></p></div></div></header><main class=main><div class=breadcrumbs><a href=/>~</a><span class=breadcrumbs-separator>/</span><a href=/projects/>Projects</a><span class=breadcrumbs-separator>/</span>
<a href=/projects/super-resolution-for-satellite-imagery/>Super Resolution for Satellite Imagery</a></div><div><article><header class=single-intro-container><h1 class=single-title>Super Resolution for Satellite Imagery</h1><p class=single-summary>Enhancing satellite imagery resolution with state-of-the-art machine learning vision models</p><div class=single-subsummary><div><p class=single-date><time datetime=2024-06-02T00:00:00+00:00>June 2, 2024</time>
&nbsp; · &nbsp;2 min read</p></div></div></header><div class=single-content><div class=table-outer><table><thead><tr><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:center><figure><div class=img-container><img loading=lazy alt="Low res" src=/projects/super-resolution-for-satellite-imagery/low-res.png></div><div class=caption-container><figcaption>Low resolution</figcaption></div></figure></td><td style=text-align:center><figure><div class=img-container><img loading=lazy alt="Ground truth high res" src=/projects/super-resolution-for-satellite-imagery/high-res.png></div><div class=caption-container><figcaption>Ground truth high resolution</figcaption></div></figure></td><td style=text-align:center><figure><div class=img-container><img loading=lazy alt="Bicubic interpolation" src=/projects/super-resolution-for-satellite-imagery/bicubic.png></div><div class=caption-container><figcaption>Bicubic interpolation</figcaption></div></figure></td></tr><tr><td style=text-align:center><figure><div class=img-container><img loading=lazy alt="MSE Autoencoder" src=/projects/super-resolution-for-satellite-imagery/ae-mse.png></div><div class=caption-container><figcaption>Autoencoder (MSE loss)</figcaption></div></figure></td><td style=text-align:center><figure><div class=img-container><img loading=lazy alt="Perceptual Autoencoder" src=/projects/super-resolution-for-satellite-imagery/ae-vgg.png></div><div class=caption-container><figcaption>Autoencoder (perceptual loss)</figcaption></div></figure></td><td style=text-align:center><figure><div class=img-container><img loading=lazy alt=SR-GAN src=/projects/super-resolution-for-satellite-imagery/srgan.png></div><div class=caption-container><figcaption>SR-GAN</figcaption></div></figure></td></tr></tbody></table></div><p>Super-resolution is the generative machine learning task of enhancing image resolution while retaining important visual structures and features. Many state-of-the-art computer vision models, such as Generative Adversarial Networks (Ledig et al., 2017), autoencoders (Dong et al., 2015), and diffusion models (Ho, Jain, and Abbeel, 2020), have been successfully applied to super-resolution tasks. These tasks have applications in many domains, such as medical imaging, computer vision, and satellite imagery (Froede, 2023). In this report, we explore these state-of-the-art (SOTA) models on satellite imagery from the Sentinel-2 and VENμS satellites.</p><p>High-resolution satellite imagery can be used for monitoring land cover changes, deforestation, and disaster response, among many other applications (Njambi, 2023). As such, it would be desirable to have a satellite that combines the strengths of both Sentinel-2 and VENμS and captures high-resolution imagery at a global scale. However, deploying a new satellite with these attributes would be costly. Instead, we apply SOTA models to bridge this gap and simulate globally available high-resolution satellite imagery.</p><p>The SEN2VENμS dataset is an open-data licensed dataset of landscape images (patches) from the Sentinel-2 and VENμS satellites which have been physically aligned and have undergone pre-processing stages. Sentinel-2 has a global range but limited resolution, whereas VENμS has a limited range, but is capable of capturing images at a higher resolution. More specifically, the Sentinel-2 images have a resolution of 128×128 pixels, whereas the VENμS images have a resolution of 256×256 pixels, so our super resolution task has an upscaling factor of two. While there are papers in the literature (Lanaras et al., 2017; Lin and Bioucas-Dias, 2020; Paris, Bioucas-Dias, and Bruzzone, 2017) that have applied super-resolution to imagery from these satellites individually, we will need to use imagery from both satellites to upscale the Sentinel-2 imagery to the resolution capability of VENμS as performed between Sentinel-2 and Rapid-Eye (Galar et al., 2019), which is a fairly novel task for our satellites (Michel et al., 2022).</p><p>This is a STAT3007 group project that I worked on in 2024 Semester 1.</p><hr><ul><li><a href=/projects/super-resolution-for-satellite-imagery/report.pdf target=_blank>Full Report</a></li><li><a href=https://github.com/LimaoC/super-resolution-for-satellite-imagery target=_blank>GitHub</a></li><li><a href=https://www.mdpi.com/2306-5729/7/7/96 target=_blank>Original dataset paper</a></li><li><a href=https://zenodo.org/records/6514159#.YoRxM5PMK3I target=_blank>Original dataset</a></li></ul></div></article><div class=single-pagination><hr><div class=flexnowrap><div class=single-pagination-prev><div class=single-pagination-container-prev><div class=single-pagination-text>←</div><div class=single-pagination-text><a href=/projects/drosophila-classification/>Drosophila Classification</a></div></div></div><div class=single-pagination-next><div class=single-pagination-container-next><div class=single-pagination-text><a href=/projects/sitting-desktop-garden/>Sitting Desktop Garden</a></div><div class=single-pagination-text>→</div></div></div></div><hr></div><div class=back-to-top><a href=#top>back to top</a></div></div></main></div><footer><p>© 2025 Limao Chang</p></footer></body><script src=/js/theme-switch.js></script><script defer src=/js/copy-code.js></script></html>